---
title: "Assignment-5"
author: "Harshini"
date: "2022-11-30"
output: html_document
---

```{r}
library(MASS)
library(ISLR)
library(glmnet)
library(leaps)
data('Boston')
```

## Best Subset Selection

```{r}
set.seed(123)
train <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (! train)
```

Here, we split train and test data using sample method

```{r}
regfit.best = regsubsets(crim ~ .,data = Boston[train,],nvmax = 13)
```

Best model subset selection are did using regsubset()

```{r}
test.mat = model.matrix(crim ~ .,data = Boston[test,])
```

create matrix form of test data

```{r}
val.errors_best = rep(NA,13)

for (i in 1:13){
  coeffient = coef(regfit.best,id=i)
  pred = test.mat[,names(coeffient)] %*% coeffient
  val.errors_best[i] = mean((Boston$crim[test]-pred)^2)
}
```

val.errors_best in that fill na values and replace all coeffienct values are there 
and find errors

```{r}
which.min(val.errors_best)
```

find minimum error 

```{r}
coef(regfit.best,2)
```

## Forward

```{r}
regfit.fwd = regsubsets(crim ~ .,data = Boston[train,],nvmax = 13, method ="forward")
```

Here use forward selection using iteration method = "forward"
and all steps are same like best subset selection 

```{r}
test.mat = model.matrix(crim ~ .,data = Boston[test,])
```


```{r}
val.errors_fwd = rep(NA,13)

for (i in 1:13){
  coeffient = coef(regfit.fwd,id=i)
  pred = test.mat[,names(coeffient)]%*%coeffient
  val.errors_fwd[i] = mean((Boston$crim[test]-pred)^2)
}
```

```{r}
which.min(val.errors_fwd)
```

```{r}
coef(regfit.fwd,2)
```

## Backward

```{r}
regfit.bwd = regsubsets(crim ~ .,data = Boston[train,],nvmax = 13, method ="backward")
```

Here use forward selection using iteration method = "backward"
and all steps are same like best subset selection

```{r}
test.mat = model.matrix(crim ~ .,data = Boston[test,])
```


```{r}
val.errors_bwd = rep(NA,13)

for (i in 1:13){
  coeffient = coef(regfit.bwd,id=i)
  pred = test.mat[,names(coeffient)]%*%coeffient
  val.errors_bwd[i] = mean((Boston$crim[test]-pred)^2)
}
```

```{r}
which.min(val.errors_bwd)
```

```{r}
coef(regfit.bwd,2)
```


## Lasso

```{r}
x = model.matrix(crim ~ .,Boston )[,-1]
y = Boston$crim
```


```{r}
set.seed(1)

train_lasso=sample (1: nrow(x), nrow(x)/2)
test_lasso=(-train_lasso) 
y.test_lasso=y[test_lasso]
```


```{r}
grid=10^seq(10,-2, length =100)

lasso.mod=glmnet(x[train_lasso ,],y[ train_lasso],alpha=1, lambda =grid)
plot(lasso.mod)
```


```{r}
set.seed(1)

cv.out=cv.glmnet(x[train_lasso ,],y[ train_lasso],alpha=1)
plot(cv.out)
```


```{r}
bestlam =cv.out$lambda.min
lasso.pred=predict (lasso.mod ,s=bestlam ,newx=x[test_lasso ,])
mean((lasso.pred -y.test_lasso)^2)
```

## Ridge

```{r}
x = model.matrix(crim ~ .,Boston )[,-1]
y = Boston$crim
```


```{r}
set.seed(1)

train_ridge=sample (1: nrow(x), nrow(x)/2)
test_ridge=(-train_ridge) 
y.test_ridge=y[test_ridge]
```



```{r}
grid=10^seq(10,-2, length =100)

ridge.mod=glmnet(x[train_ridge ,],y[ train_ridge],alpha=0, lambda =grid)
plot(ridge.mod)
```


```{r}
set.seed(1)

cv.out=cv.glmnet(x[train_ridge ,],y[ train_ridge],alpha=0)
plot(cv.out)
```


```{r}
bestlam =cv.out$lambda.min
ridge.pred=predict (ridge.mod ,s=bestlam ,newx=x[test_ridge ,])
mean((ridge.pred -y.test_ridge)^2)
```

ANSWER : Here I conclude that best subset selection gives minimum error compared to lasso and ridge selection  

